* 00
[[~/ql/experiments/00/w.png]]
** O quê: Reprodução do artigo
** Resultado: diverso do esperado
   O Q-Learning apresentou um desempenho extremamente melhor do que o PRQL.
   Houve um problema: não estava zerando as Q-Tables (QLearning e PRQLearning)
	 

* 01
[[~/ql/experiments/01/w.png]]
Repetição de 00 após correção do problema
Resultado: PRQL tem o mesmo comportamento de QL, só um pouco pior.
Hipótese: não está utilizando as políticas antigas


* 02 QL vs PRQL no mundo 05x05
[[~/ql/experiments/02/w.png]]
Motivo: queria rodar o PRQL sem a parte PR, ou seja, só utilizando
QLearning, pra ver se estava tudo OK
Nesse experimento, NÃO utilizava pi-reuse, somente QL


* 03 Mesmo experimento de 02, só que para o mundo 06x06


* 04 Mesmo experimento de 02, só que para a task omega do artigo
[[~/ql/experiments/04/w.png]]
** Resultado:
   PRQL e QL apresentaram desempenhos compatíveis, o que era esperado


* 05 Repetindo 04, só que dessa vez ativando o pi-reuse
[[~/ql/experiments/05/w.png]]
  Sucesso: PRQL acelerou QLearning


* 06 Repetindo 05 para task omega do artigo reutilizando políticas 2, 3 e 5 (são as que mais ajudam o agente)
[[~/ql/experiments/06/w.png]]
  Problema: plotando W[ 1]


* 07 Repetindo 06
[[~/ql/experiments/07/w.png]]
  Problema: plotando W[ 1]


* 08 Repetindo 06, mas reutilizando somente a política ótima
[[~/ql/experiments/08/w.png]]
  Problema: plotando W[ 1]


* 09 Repetindo 02, após correção do acúmulo de recompensas médias por episódio
[[~/ql/experiments/09/w.png]]
Problema: reutilizando políticas subótimas


* 10 Repetindo 09, mas reutilizando uma política ótima para o problema de chegar
  à localização oposta (pior política que poderia reutilizar)
[[~/ql/experiments/10/w.png]]
Problema: reutilizando políticas subótimas


* 11 Resolver task omega utilizando pols. 2,3,4,5
[[~/ql/experiments/11/w.png]]
Problema: reutilizando políticas subótimas


* 12 Repetindo 11 reutilizando somente a policy obtida em 11 pelo
  QLearning (ótima para o problema)
[[~/ql/experiments/12/w.png]]
Problema: reutilizando políticas subótimas


* 13 Repetindo 12, só que chamei o solveMDP... pra criar os arquivos (tirar a dúvida se
  arquivos estão corretos)
Problema: reutilizando políticas subótimas


* 14 Repetição do 13, só que agora utilizando a política ótima
  Resultado: PRQL aprende mais rápido no começo, mas converge em ~ 1500 episódios para 
W_PRQL ~ 0.12 < W_QL ~ 0.14.

  A impressão que dá é que o agente "pára" de aprender quando está executando o PRQL
