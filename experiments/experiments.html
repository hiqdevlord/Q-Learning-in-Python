<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
lang="en" xml:lang="en">
<head>
<title>experiments</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2012-04-10 16:06:18 BRT"/>
<meta name="author" content="Rafael Lemes Beirigo"/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color:lightblue; font-weight:normal }
  .target { }
  .timestamp { color: grey }
  .timestamp-kwd { color: CadetBlue }
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*/-->
</script>
</head><body>
<h1 class="title">experiments</h1>


<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1 Experimento 00 Experimento da seção 4.3 do artigo: resolver task &Omega; </a>
<ul>
<li><a href="#sec-1.1">1.1 01 Utilizando &Pi;<sub>1</sub>, &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub>, &Pi;<sub>5</sub> </a></li>
<li><a href="#sec-1.2">1.2 02 Utilizando &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub> </a></li>
<li><a href="#sec-1.3">1.3 03 Utilizando &Pi;<sub>1</sub>, &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub> </a></li>
<li><a href="#sec-1.4">1.4 04 Utilizando &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub>, &Pi;<sub>5</sub> </a></li>
<li><a href="#sec-1.5">1.5 Discussão </a></li>
</ul>
</li>
<li><a href="#sec-2">2 Experimento 01 </a>
<ul>
<li><a href="#sec-2.1">2.1 01 Utilizando &Pi;<sub>1</sub>, &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub>, &Pi;<sub>5</sub> </a></li>
<li><a href="#sec-2.2">2.2 02 Utilizando &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub> </a></li>
<li><a href="#sec-2.3">2.3 03 Utilizando &Pi;<sub>1</sub>, &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub> </a></li>
<li><a href="#sec-2.4">2.4 04 Utilizando &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub>, &Pi;<sub>5</sub> </a></li>
<li><a href="#sec-2.5">2.5 Discussão </a></li>
</ul>
</li>
<li><a href="#sec-3">3 Experimento 02 QL vs PRQL no mundo 05x05 </a>
<ul>
<li><a href="#sec-3.1">3.1 Discussão </a></li>
</ul>
</li>
<li><a href="#sec-4">4 03 Repetição de 02, só que para o mundo 06x06 </a>
<ul>
<li><a href="#sec-4.1">4.1 Discussão </a></li>
</ul>
</li>
<li><a href="#sec-5">5 04 Repetição de 02, só que para a task &Omega; do artigo </a>
<ul>
<li><a href="#sec-5.1">5.1 Discussão: </a></li>
</ul>
</li>
<li><a href="#sec-6">6 05 Repetição de 04, só que dessa vez ativando o &pi;-reuse </a>
<ul>
<li><a href="#sec-6.1">6.1 Discussão </a></li>
</ul>
</li>
<li><a href="#sec-7">7 06 Repetição de 05 para task &Omega; do artigo reutilizando &Pi;<sub>2</sub>, &Pi;<sub>3</sub> e &Pi;<sub>5</sub> (são as que mais ajudam o agente) </a>
<ul>
<li><a href="#sec-7.1">7.1 Discussão </a></li>
</ul>
</li>
<li><a href="#sec-8">8 07 Repetição de 06 </a>
<ul>
<li><a href="#sec-8.1">8.1 Discussão </a></li>
</ul>
</li>
<li><a href="#sec-9">9 08 Repetição de 06, mas reutilizando somente a política ótima </a>
<ul>
<li><a href="#sec-9.1">9.1 Discussão </a></li>
</ul>
</li>
<li><a href="#sec-10">10 09 Repetição de 02 </a>
<ul>
<li><a href="#sec-10.1">10.1 Discussão </a></li>
</ul>
</li>
<li><a href="#sec-11">11 10 Repetição de 09, mas reutilizando uma política ótima para o problema de chegar à localização oposta (pior política que poderia reutilizar) </a>
<ul>
<li><a href="#sec-11.1">11.1 Discussão </a></li>
</ul>
</li>
<li><a href="#sec-12">12 11 Resolver task &Omega; utilizando  &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub>, &Pi;<sub>5</sub> </a>
<ul>
<li><a href="#sec-12.1">12.1 Discussão </a></li>
</ul>
</li>
<li><a href="#sec-13">13 12 Repetição de 11 reutilizando somente a policy obtida em 11 pelo QLearning (ótima para o problema) </a></li>
<li><a href="#sec-14">14 13 Repetição de 12, só que chamei o solveMDP&hellip; pra criar os arquivos (tirar a dúvida se </a></li>
<li><a href="#sec-15">15 14 Repetição do 13, só que agora utilizando a política ótima </a></li>
<li><a href="#sec-16">16 15 Obtenção de &Pi;<sub>1</sub> </a></li>
<li><a href="#sec-17">17 16 Obtenção de &Pi;<sub>2</sub> </a></li>
<li><a href="#sec-18">18 17 Obtenção de &Pi;<sub>3</sub> </a></li>
<li><a href="#sec-19">19 18 Obtenção de &Pi;<sub>4</sub> </a></li>
<li><a href="#sec-20">20 19 Obtenção de &Pi;<sub>5</sub> </a></li>
<li><a href="#sec-21">21 20 Resolver task &Omega; utilizando &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub>, &Pi;<sub>5</sub> (Repetição do 11) </a></li>
<li><a href="#sec-22">22 21 Resolver task &Omega; utilizando &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub> </a></li>
<li><a href="#sec-23">23 22 Resolver task &Omega; utilizando &Pi;<sub>1</sub>, &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub> </a></li>
<li><a href="#sec-24">24 23 Repetição do 02 </a></li>
<li><a href="#sec-25">25 24 Repetição do 22 </a></li>
<li><a href="#sec-26">26 25 Repetição do 20 </a></li>
<li><a href="#sec-27">27 26 Repetição do 21 </a></li>
<li><a href="#sec-28">28 Aprendizado de &Pi;<sub>&Omega;</sub> com reutilização individual de políticas </a>
<ul>
<li><a href="#sec-28.1">28.1 27 Reutilizando &Pi;<sub>1</sub> </a></li>
<li><a href="#sec-28.2">28.2 28 Reutilizando &Pi;<sub>2</sub> </a></li>
<li><a href="#sec-28.3">28.3 29 Reutilizando &Pi;<sub>3</sub> </a></li>
<li><a href="#sec-28.4">28.4 30 Reutilizando &Pi;<sub>4</sub> </a></li>
<li><a href="#sec-28.5">28.5 31 Reutilizando &Pi;<sub>5</sub> </a></li>
</ul>
</li>
<li><a href="#sec-29">29 Resolver task1 utilizando &pi;-reuse(&Pi;<sub>1</sub>) </a>
<ul>
<li><a href="#sec-29.1">29.1 10 execuções </a>
<ul>
<li><a href="#sec-29.1.1">29.1.1 Experimento 32 - em 100% dos episódios - 10 execuções </a></li>
<li><a href="#sec-29.1.2">29.1.2 Experimento 33 - em  75% dos episódios - 10 execuções </a></li>
<li><a href="#sec-29.1.3">29.1.3 Experimento 34 - em  50% dos episódios - 10 execuções </a></li>
<li><a href="#sec-29.1.4">29.1.4 Experimento 35 - em  25% dos episódios - 10 execuções </a></li>
<li><a href="#sec-29.1.5">29.1.5 Experimento 36 - em   0% dos episódios - 10 execuções </a></li>
</ul>
</li>
<li><a href="#sec-29.2">29.2 100 execuções </a>
<ul>
<li><a href="#sec-29.2.1">29.2.1 Experimento 37 - em   0% dos episódios </a></li>
<li><a href="#sec-29.2.2">29.2.2 Experimento 38 - em  25% dos episódios </a></li>
<li><a href="#sec-29.2.3">29.2.3 Experimento 39 - em  50% dos episódios </a></li>
<li><a href="#sec-29.2.4">29.2.4 Experimento 40 - em  75% dos episódios </a></li>
<li><a href="#sec-29.2.5">29.2.5 Experimento 41 - em 100% dos episódios </a></li>
</ul></li>
</ul>
</li>
<li><a href="#sec-30">30 IPMU </a>
<ul>
<li><a href="#sec-30.1">30.1 42 Com arquivos originais - 2000 episódios, 100 passos </a></li>
<li><a href="#sec-30.2">30.2 43 Com arquivos originais - 1e05 episódios, 1000 passos </a></li>
<li><a href="#sec-30.3">30.3 44 Modifiquei o transitions.in </a></li>
<li><a href="#sec-30.4">30.4 78 Reutilizando políticas probabilísticas </a>
<ul>
<li><a href="#sec-30.4.1">30.4.1 PRQL </a></li>
<li><a href="#sec-30.4.2">30.4.2 QL </a></li>
</ul></li>
</ul>
</li>
<li><a href="#sec-31">31 Testes da versão probabilística do PRQL (PRQL<sub>prob</sub>) </a>
<ul>
<li><a href="#sec-31.1">31.1 Reutilizar uma política determinística ótima e sua versão probabilística (1.0 de probabilidade para cada ação ótima) </a>
<ul>
<li><a href="#sec-31.1.1">31.1.1 Experimentos </a></li>
<li><a href="#sec-31.1.2">31.1.2 Discussão: </a></li>
</ul>
</li>
<li><a href="#sec-31.2">31.2 Experimentos de 51 a 55: repetição dos experimentos de 27 a 31 utilizando o PRQL probabilístico </a>
<ul>
<li><a href="#sec-31.2.1">31.2.1 As políticas utilizadas são as ótimas com 1.0 para cada ação da política </a></li>
<li><a href="#sec-31.2.2">31.2.2 Utilizando como referência aprendizado com QL do Experimento 37 </a></li>
</ul>
</li>
<li><a href="#sec-31.3">31.3 Experimentos de 56 a 66: teste de utilização incremental de política probabilística ótima versus <i>péssima</i> </a>
<ul>
<li><a href="#sec-31.3.1">31.3.1 Problema: resolver task1 utilizando percentuais variáveis das políticas ótima e péssima </a></li>
<li><a href="#sec-31.3.2">31.3.2 Descrição: Rodar o PRQL com aproveitamento da política ótima e <i>péssima</i>, onde a <i>péssima</i> sugere sempre o oposto da ótima. </a></li>
<li><a href="#sec-31.3.3">31.3.3 A utilização da política ótima varia linearmente com o experimento, de 0% (Experimento 56) a 100% (Experimento 66). </a></li>
<li><a href="#sec-31.3.4">31.3.4 QLearning: foi realizado um novo aprendizado de Q-Learning para o experimento </a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1">1 Experimento 00 Experimento da seção 4.3 do artigo: resolver task &Omega; </h2>
<div id="text-1">


</div>

<div id="outline-container-1.1" class="outline-3">
<h3 id="sec-1.1">1.1 01 Utilizando &Pi;<sub>1</sub>, &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub>, &Pi;<sub>5</sub> </h3>
<div id="text-1.1">

<p>Esse experimento não está no artigo. Fiz para fins de testes.
</p>

<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/00/01/map.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/00/01/map.png" /></p>
</div>


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/00/01/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/00/01/w.png" /></p>
</div>

</div>

</div>

<div id="outline-container-1.2" class="outline-3">
<h3 id="sec-1.2">1.2 02 Utilizando &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub> </h3>
<div id="text-1.2">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/00/02/map.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/00/02/map.png" /></p>
</div>


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/00/02/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/00/02/w.png" /></p>
</div>

</div>

</div>

<div id="outline-container-1.3" class="outline-3">
<h3 id="sec-1.3">1.3 03 Utilizando &Pi;<sub>1</sub>, &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub> </h3>
<div id="text-1.3">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/00/03/map.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/00/03/map.png" /></p>
</div>


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/00/03/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/00/03/w.png" /></p>
</div>

</div>

</div>

<div id="outline-container-1.4" class="outline-3">
<h3 id="sec-1.4">1.4 04 Utilizando &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub>, &Pi;<sub>5</sub> </h3>
<div id="text-1.4">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/00/04/map.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/00/04/map.png" /></p>
</div>


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/00/04/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/00/04/w.png" /></p>
</div>

</div>

</div>

<div id="outline-container-1.5" class="outline-3">
<h3 id="sec-1.5">1.5 Discussão </h3>
<div id="text-1.5">

<p>Q-Learning apresentou um desempenho superior ao de PRQ-Learning.
Posteriormente descobri que o motivo era que a tabela Q era reiniciada somente para o 
PRQ-Learning, o que prejudicava o desempenho desse algoritmo.
</p>

</div>
</div>

</div>

<div id="outline-container-2" class="outline-2">
<h2 id="sec-2">2 Experimento 01 </h2>
<div id="text-2">

<p>Repetição de 00 após correção do problema.
</p>
</div>

<div id="outline-container-2.1" class="outline-3">
<h3 id="sec-2.1">2.1 01 Utilizando &Pi;<sub>1</sub>, &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub>, &Pi;<sub>5</sub> </h3>
<div id="text-2.1">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/00/01/map.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/00/01/map.png" /></p>
</div>


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/01/01/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/01/01/w.png" /></p>
</div>

</div>

</div>

<div id="outline-container-2.2" class="outline-3">
<h3 id="sec-2.2">2.2 02 Utilizando &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub> </h3>
<div id="text-2.2">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/00/02/map.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/00/02/map.png" /></p>
</div>


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/01/02/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/01/02/w.png" /></p>
</div>

</div>

</div>

<div id="outline-container-2.3" class="outline-3">
<h3 id="sec-2.3">2.3 03 Utilizando &Pi;<sub>1</sub>, &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub> </h3>
<div id="text-2.3">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/00/03/map.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/00/03/map.png" /></p>
</div>


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/01/03/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/01/03/w.png" /></p>
</div>

</div>

</div>

<div id="outline-container-2.4" class="outline-3">
<h3 id="sec-2.4">2.4 04 Utilizando &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub>, &Pi;<sub>5</sub> </h3>
<div id="text-2.4">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/00/04/map.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/00/04/map.png" /></p>
</div>


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/01/04/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/01/04/w.png" /></p>
</div>

</div>

</div>

<div id="outline-container-2.5" class="outline-3">
<h3 id="sec-2.5">2.5 Discussão </h3>
<div id="text-2.5">

<p>O PRQL apresenta um desempenho similar ao do QL.
O resultado não foi o esperado: esperava-se um desempenho melhor por parte do PRQL,
dado que esse possui a reutilização de políticas.
Hipótese: não está utilizando as políticas antigas
</p>

</div>
</div>

</div>

<div id="outline-container-3" class="outline-2">
<h2 id="sec-3">3 Experimento 02 QL vs PRQL no mundo 05x05 </h2>
<div id="text-3">

<p>Motivo: queria rodar o PRQL sem a parte PR, ou seja, só utilizando
QLearning, pra ver se estava tudo OK.
</p>
<p>
Nesse experimento, o PRQL NÃO utilizava &pi;-reuse, somente QL
</p>

<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/02/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/02/w.png" /></p>
</div>


</div>

<div id="outline-container-3.1" class="outline-3">
<h3 id="sec-3.1">3.1 Discussão </h3>
<div id="text-3.1">

<p>PRQL apresentou um desempenho bastante similar ao do QL, o que sugere que a execução
do PRQL sem &pi;-reuse equivale à execução do QL.
</p>
<p>
Isso já era esperado, pois o elemento do PRQL que acelera o aprendizado é justamente
o &pi;-reuse
</p>

</div>
</div>

</div>

<div id="outline-container-4" class="outline-2">
<h2 id="sec-4">4 03 Repetição de 02, só que para o mundo 06x06 </h2>
<div id="text-4">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/03/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/03/w.png" /></p>
</div>


</div>

<div id="outline-container-4.1" class="outline-3">
<h3 id="sec-4.1">4.1 Discussão </h3>
<div id="text-4.1">

<p>O resultado foi compatível com o obtido no experimento 02, conforme esperado
</p>

</div>
</div>

</div>

<div id="outline-container-5" class="outline-2">
<h2 id="sec-5">5 04 Repetição de 02, só que para a task &Omega; do artigo </h2>
<div id="text-5">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/04/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/04/w.png" /></p>
</div>


</div>

<div id="outline-container-5.1" class="outline-3">
<h3 id="sec-5.1">5.1 Discussão: </h3>
<div id="text-5.1">

<p>PRQL e QL apresentaram desempenhos compatíveis, o que era esperado
</p>

</div>
</div>

</div>

<div id="outline-container-6" class="outline-2">
<h2 id="sec-6">6 05 Repetição de 04, só que dessa vez ativando o &pi;-reuse </h2>
<div id="text-6">

<p>A bilioteca de políticas continha somente a &Pi;<sup>*</sup><sub>&Omega;</sub>.
</p>

<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/05/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/05/w.png" /></p>
</div>
<p>
Sucesso: PRQL acelerou QLearning
</p>

</div>

<div id="outline-container-6.1" class="outline-3">
<h3 id="sec-6.1">6.1 Discussão </h3>
<div id="text-6.1">

<p>O elevado desempenho do PRQL pode ser explicado pelo fato de que a
política utilizada é justamente a ótima para o problema.
</p>

</div>
</div>

</div>

<div id="outline-container-7" class="outline-2">
<h2 id="sec-7">7 06 Repetição de 05 para task &Omega; do artigo reutilizando &Pi;<sub>2</sub>, &Pi;<sub>3</sub> e &Pi;<sub>5</sub> (são as que mais ajudam o agente) </h2>
<div id="text-7">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/06/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/06/w.png" /></p>
</div>


</div>

<div id="outline-container-7.1" class="outline-3">
<h3 id="sec-7.1">7.1 Discussão </h3>
<div id="text-7.1">

<p>Problema: plotando W[ 1]
</p>
<p>
Foi plotado somente a recompensa acumulada quando se reutilizava uma
das políticas possíveis, L[ 1]para uma das políticas
reutilizadas.
</p>
<p>
Como espera-se um aumento gradual da utilização da política &Pi;<sub>new</sub>, e
a recompensa acumulada pela utilização de &Pi;<sub>new</sub> se encontra em W[
0], o valor plotado em W[ 1] não reflete o que esperamos.
</p>

</div>
</div>

</div>

<div id="outline-container-8" class="outline-2">
<h2 id="sec-8">8 07 Repetição de 06 </h2>
<div id="text-8">

<p>A repetição foi feita para testes
</p>

<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/07/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/07/w.png" /></p>
</div>


</div>

<div id="outline-container-8.1" class="outline-3">
<h3 id="sec-8.1">8.1 Discussão </h3>
<div id="text-8.1">

<p>Problema: plotando W[ 1]
A repetição foi feita antes da detecção do problema descrito em 05.
</p>

</div>
</div>

</div>

<div id="outline-container-9" class="outline-2">
<h2 id="sec-9">9 08 Repetição de 06, mas reutilizando somente a política ótima </h2>
<div id="text-9">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/08/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/08/w.png" /></p>
</div>


</div>

<div id="outline-container-9.1" class="outline-3">
<h3 id="sec-9.1">9.1 Discussão </h3>
<div id="text-9.1">

<p>O resultado foi diverso do esperado.
</p>
<p>
A recompensa acumulada estaciona em ~ 0.13, um valor extremamente
baixo, superado pelo Q-Learning durante os experimentos.
</p>

</div>
</div>

</div>

<div id="outline-container-10" class="outline-2">
<h2 id="sec-10">10 09 Repetição de 02 </h2>
<div id="text-10">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/02/w.png" title="02" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/02/w.png" /></p>
</div>


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/09/w.png" title="09" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/09/w.png" /></p>
</div>


</div>

<div id="outline-container-10.1" class="outline-3">
<h3 id="sec-10.1">10.1 Discussão </h3>
<div id="text-10.1">

<p>O desempenho do PRQL aumentou em relação ao experimento 02. Isso pode
ser explicado pelo fato de que foi utilizado &pi;-reuse nesse
experimento, o que contribui para acelerar o aprendizado.
</p>

</div>
</div>

</div>

<div id="outline-container-11" class="outline-2">
<h2 id="sec-11">11 10 Repetição de 09, mas reutilizando uma política ótima para o problema de chegar à localização oposta (pior política que poderia reutilizar) </h2>
<div id="text-11">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/09/w.png" title="09" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/09/w.png" /></p>
</div>


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/10/w.png" title="10" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/10/w.png" /></p>
</div>


</div>

<div id="outline-container-11.1" class="outline-3">
<h3 id="sec-11.1">11.1 Discussão </h3>
<div id="text-11.1">

<p>O resultado foi de acordo com o esperado, pois o desempenho do PRQL
cai quando a política que está sendo reutilizada atrapalharia na
solução do problema.
</p>

</div>
</div>

</div>

<div id="outline-container-12" class="outline-2">
<h2 id="sec-12">12 11 Resolver task &Omega; utilizando  &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub>, &Pi;<sub>5</sub> </h2>
<div id="text-12">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/11/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/11/w.png" /></p>
</div>


</div>

<div id="outline-container-12.1" class="outline-3">
<h3 id="sec-12.1">12.1 Discussão </h3>
<div id="text-12.1">

<p>PRQL apresenta desempenho inferior ao de QL, o oposto do esperado.
</p>

</div>
</div>

</div>

<div id="outline-container-13" class="outline-2">
<h2 id="sec-13">13 12 Repetição de 11 reutilizando somente a policy obtida em 11 pelo QLearning (ótima para o problema) </h2>
<div id="text-13">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/12/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/12/w.png" /></p>
</div>



</div>

</div>

<div id="outline-container-14" class="outline-2">
<h2 id="sec-14">14 13 Repetição de 12, só que chamei o solveMDP&hellip; pra criar os arquivos (tirar a dúvida se </h2>
<div id="text-14">

<p>arquivos estão corretos)
</p>
<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/13/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/13/w.png" /></p>
</div>
<p>
Pude perceber a partir desse experimento que as políticas que estavam
sendo reutilizadas eram subótimas.
</p>

</div>

</div>

<div id="outline-container-15" class="outline-2">
<h2 id="sec-15">15 14 Repetição do 13, só que agora utilizando a política ótima </h2>
<div id="text-15">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/14/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/14/w.png" /></p>
</div>


</div>

</div>

<div id="outline-container-16" class="outline-2">
<h2 id="sec-16">16 15 Obtenção de &Pi;<sub>1</sub> </h2>
<div id="text-16">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/15/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/15/w.png" /></p>
</div>
<p>
Consumo de tempo: ~ 10'
</p>

</div>

</div>

<div id="outline-container-17" class="outline-2">
<h2 id="sec-17">17 16 Obtenção de &Pi;<sub>2</sub> </h2>
<div id="text-17">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/16/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/16/w.png" /></p>
</div>
<p>
Consumo de tempo: ~ 10'
</p>

</div>

</div>

<div id="outline-container-18" class="outline-2">
<h2 id="sec-18">18 17 Obtenção de &Pi;<sub>3</sub> </h2>
<div id="text-18">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/17/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/17/w.png" /></p>
</div>
<p>
Consumo de tempo: ~ 10'
</p>

</div>

</div>

<div id="outline-container-19" class="outline-2">
<h2 id="sec-19">19 18 Obtenção de &Pi;<sub>4</sub> </h2>
<div id="text-19">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/18/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/18/w.png" /></p>
</div>
<p>
Consumo de tempo: ~ 10'
</p>

</div>

</div>

<div id="outline-container-20" class="outline-2">
<h2 id="sec-20">20 19 Obtenção de &Pi;<sub>5</sub> </h2>
<div id="text-20">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/19/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/19/w.png" /></p>
</div>
<p>
Consumo de tempo: ~ 10'
</p>

</div>

</div>

<div id="outline-container-21" class="outline-2">
<h2 id="sec-21">21 20 Resolver task &Omega; utilizando &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub>, &Pi;<sub>5</sub> (Repetição do 11) </h2>
<div id="text-21">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/20/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/20/w.png" /></p>
</div>


</div>

</div>

<div id="outline-container-22" class="outline-2">
<h2 id="sec-22">22 21 Resolver task &Omega; utilizando &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub> </h2>
<div id="text-22">



</div>

</div>

<div id="outline-container-23" class="outline-2">
<h2 id="sec-23">23 22 Resolver task &Omega; utilizando &Pi;<sub>1</sub>, &Pi;<sub>2</sub>, &Pi;<sub>3</sub>, &Pi;<sub>4</sub> </h2>
<div id="text-23">



</div>

</div>

<div id="outline-container-24" class="outline-2">
<h2 id="sec-24">24 23 Repetição do 02 </h2>
<div id="text-24">



</div>

</div>

<div id="outline-container-25" class="outline-2">
<h2 id="sec-25">25 24 Repetição do 22 </h2>
<div id="text-25">



</div>

</div>

<div id="outline-container-26" class="outline-2">
<h2 id="sec-26">26 25 Repetição do 20 </h2>
<div id="text-26">



</div>

</div>

<div id="outline-container-27" class="outline-2">
<h2 id="sec-27">27 26 Repetição do 21 </h2>
<div id="text-27">



</div>

</div>

<div id="outline-container-28" class="outline-2">
<h2 id="sec-28">28 Aprendizado de &Pi;<sub>&Omega;</sub> com reutilização individual de políticas </h2>
<div id="text-28">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/27/map.png" height="240" style="border:2px solid black;" height="240" style="border:2px solid black;" height="240" style="border:2px solid black;" height="240" style="border:2px solid black;" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/27/map.png" /></p>
</div>


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/27/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/27/w.png" /></p>
</div>

<p>
No gráfico, os valores referentes ao Q-Learning foram obtidos no experimento 37.
</p>

</div>

<div id="outline-container-28.1" class="outline-3">
<h3 id="sec-28.1">28.1 27 Reutilizando &Pi;<sub>1</sub> </h3>
<div id="text-28.1">

<p>Consumo de tempo: 5m20.356s
</p></div>

</div>

<div id="outline-container-28.2" class="outline-3">
<h3 id="sec-28.2">28.2 28 Reutilizando &Pi;<sub>2</sub> </h3>
<div id="text-28.2">

<p>Consumo de tempo: 7m53.056s
</p></div>

</div>

<div id="outline-container-28.3" class="outline-3">
<h3 id="sec-28.3">28.3 29 Reutilizando &Pi;<sub>3</sub> </h3>
<div id="text-28.3">

<p>Consumo de tempo: 9m8.582s
</p></div>

</div>

<div id="outline-container-28.4" class="outline-3">
<h3 id="sec-28.4">28.4 30 Reutilizando &Pi;<sub>4</sub> </h3>
<div id="text-28.4">

<p>Consumo de tempo: 10m19.403s
</p></div>

</div>

<div id="outline-container-28.5" class="outline-3">
<h3 id="sec-28.5">28.5 31 Reutilizando &Pi;<sub>5</sub> </h3>
<div id="text-28.5">

<p>Consumo de tempo: 6m8.686s
</p>

</div>
</div>

</div>

<div id="outline-container-29" class="outline-2">
<h2 id="sec-29">29 Resolver task1 utilizando &pi;-reuse(&Pi;<sub>1</sub>) </h2>
<div id="text-29">


</div>

<div id="outline-container-29.1" class="outline-3">
<h3 id="sec-29.1">29.1 10 execuções </h3>
<div id="text-29.1">

<p>Política reutilizada: &Pi;<sub>1</sub>, obtida no experimento 15
</p>
<p>
w.32.png - dados do Q-Learning obtidos no experimento 32
</p>
<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/32/w.32.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/32/w.32.png" /></p>
</div>

<p>
w.37.png - dados do Q-Learning obtidos no experimento 37
</p>
<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/32/w.37.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/32/w.37.png" /></p>
</div>


</div>

<div id="outline-container-29.1.1" class="outline-4">
<h4 id="sec-29.1.1">29.1.1 Experimento 32 - em 100% dos episódios - 10 execuções </h4>
<div id="text-29.1.1">

</div>

</div>

<div id="outline-container-29.1.2" class="outline-4">
<h4 id="sec-29.1.2">29.1.2 Experimento 33 - em  75% dos episódios - 10 execuções </h4>
<div id="text-29.1.2">

</div>

</div>

<div id="outline-container-29.1.3" class="outline-4">
<h4 id="sec-29.1.3">29.1.3 Experimento 34 - em  50% dos episódios - 10 execuções </h4>
<div id="text-29.1.3">

</div>

</div>

<div id="outline-container-29.1.4" class="outline-4">
<h4 id="sec-29.1.4">29.1.4 Experimento 35 - em  25% dos episódios - 10 execuções </h4>
<div id="text-29.1.4">

</div>

</div>

<div id="outline-container-29.1.5" class="outline-4">
<h4 id="sec-29.1.5">29.1.5 Experimento 36 - em   0% dos episódios - 10 execuções </h4>
<div id="text-29.1.5">


</div>
</div>

</div>

<div id="outline-container-29.2" class="outline-3">
<h3 id="sec-29.2">29.2 100 execuções </h3>
<div id="text-29.2">


<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/37/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/37/w.png" /></p>
</div>

<p>
Política reutilizada: &Pi;<sub>1</sub>, obtida no experimento 15
</p>
<p>
Dados do Q-Learning obtidos no experimento 37
</p>

</div>

<div id="outline-container-29.2.1" class="outline-4">
<h4 id="sec-29.2.1">29.2.1 Experimento 37 - em   0% dos episódios </h4>
<div id="text-29.2.1">

</div>

</div>

<div id="outline-container-29.2.2" class="outline-4">
<h4 id="sec-29.2.2">29.2.2 Experimento 38 - em  25% dos episódios </h4>
<div id="text-29.2.2">

</div>

</div>

<div id="outline-container-29.2.3" class="outline-4">
<h4 id="sec-29.2.3">29.2.3 Experimento 39 - em  50% dos episódios </h4>
<div id="text-29.2.3">

</div>

</div>

<div id="outline-container-29.2.4" class="outline-4">
<h4 id="sec-29.2.4">29.2.4 Experimento 40 - em  75% dos episódios </h4>
<div id="text-29.2.4">

</div>

</div>

<div id="outline-container-29.2.5" class="outline-4">
<h4 id="sec-29.2.5">29.2.5 Experimento 41 - em 100% dos episódios </h4>
<div id="text-29.2.5">



</div>
</div>
</div>

</div>

<div id="outline-container-30" class="outline-2">
<h2 id="sec-30">30 IPMU </h2>
<div id="text-30">


</div>

<div id="outline-container-30.1" class="outline-3">
<h3 id="sec-30.1">30.1 42 Com arquivos originais - 2000 episódios, 100 passos </h3>
<div id="text-30.1">

<p>w.out completamente zerado.
</p>
</div>

</div>

<div id="outline-container-30.2" class="outline-3">
<h3 id="sec-30.2">30.2 43 Com arquivos originais - 1e05 episódios, 1000 passos </h3>
<div id="text-30.2">

<p>Supondo que o problema relatado em 42 fosse a quantidade de episódios
e/ou passos, rodei novamente, com 
</p>
</div>

</div>

<div id="outline-container-30.3" class="outline-3">
<h3 id="sec-30.3">30.3 44 Modifiquei o transitions.in </h3>
<div id="text-30.3">

<p>Modificação realizada:
Para cada linha do arquivo, o valor da transição foi alterado para
1.0 / 136, sendo que 136 é o número total de estados.
Com isso, espero ter um grafo completo de transições, logo, poderei
verificar se foi esse o problema que impediu o agente de receber
recompensas nos experimentos 42 e 43.
Vale notar que estou supondo que existe uma linha s a s' t para todas
as combinações de s a s' possíveis.
É bem provável que isso seja verdade já que a quantidade de linhas do
arquivo transitions.in é 73984 = 136 * 4 * 136 (|S| * |A| * |S|).
</p>
</div>

</div>

<div id="outline-container-30.4" class="outline-3">
<h3 id="sec-30.4">30.4 78 Reutilizando políticas probabilísticas </h3>
<div id="text-30.4">


</div>

<div id="outline-container-30.4.1" class="outline-4">
<h4 id="sec-30.4.1">30.4.1 PRQL </h4>
<div id="text-30.4.1">

<ul>
<li id="sec-30.4.1.1"><i>prob</i>: reutilizando pol. prob. enviada pelo Marcelo <br/>
</li>
<li id="sec-30.4.1.2"><i>prob.det</i>: pol. ótima induzida pela pol. enviada pelo Marcelo <br/>
</li>
<li id="sec-30.4.1.3"><i>det</i>: reutilizando pol. ótima induzida, só que na versão antiga do PRQL (a versão determinística) <br/>
</li>
</ul>
</div>

</div>

<div id="outline-container-30.4.2" class="outline-4">
<h4 id="sec-30.4.2">30.4.2 QL </h4>
<div id="text-30.4.2">



</div>
</div>
</div>

</div>

<div id="outline-container-31" class="outline-2">
<h2 id="sec-31">31 Testes da versão probabilística do PRQL (PRQL<sub>prob</sub>) </h2>
<div id="text-31">


</div>

<div id="outline-container-31.1" class="outline-3">
<h3 id="sec-31.1">31.1 Reutilizar uma política determinística ótima e sua versão probabilística (1.0 de probabilidade para cada ação ótima) </h3>
<div id="text-31.1">



<div class="figure">
<p><img src="/home/rafaelbeirigo/ql/experiments/49/w.png" height="240" style="border:2px solid black;" alt="/home/rafaelbeirigo/ql/experiments/49/w.png" /></p>
</div>


</div>

<div id="outline-container-31.1.1" class="outline-4">
<h4 id="sec-31.1.1">31.1.1 Experimentos </h4>
<div id="text-31.1.1">

<ul>
<li id="sec-31.1.1.1">49 <br/>
<ul>
<li id="sec-31.1.1.1.1">Algoritmo: PRQL<sub>prob</sub> <br/>
</li>
<li id="sec-31.1.1.1.2">Task: 1 <br/>
</li>
<li id="sec-31.1.1.1.3">Políticas reutilizadas: &Pi;<sub>1</sub><sup>*</sup><sub>prob1</sub> <br/>
<ul>
<li id="sec-31.1.1.1.3.1">&Pi;<sub>1</sub><sup>*</sup><sub>prob1</sub> foi obtida colocando 1.0 em cada linha da política ótima determinística induzida por &Pi;<sub>1</sub><sup>*</sup><sub>prob</sub> <br/>
</li>
</ul>
</li>
<li id="sec-31.1.1.1.4">Parâmetros: <a href="/home/rafaelbeirigo/ql/experiments/49/PRQL/parameters.out">/home/rafaelbeirigo/ql/experiments/49/PRQL/parameters.out</a> <br/>

</li>
</ul>
</li>
<li id="sec-31.1.1.2">50 <br/>
<ul>
<li id="sec-31.1.1.2.1">Algoritmo: PRQL <br/>
</li>
<li id="sec-31.1.1.2.2">Task: 1 <br/>
</li>
<li id="sec-31.1.1.2.3">Políticas reutilizadas: &Pi;<sub>1</sub><sup>*</sup> <br/>
</li>
<li id="sec-31.1.1.2.4">Parâmetros: <a href="/home/rafaelbeirigo/ql/experiments/50/PRQL/parameters.out">/home/rafaelbeirigo/ql/experiments/50/PRQL/parameters.out</a> <br/>

</li>
</ul>
</li>
</ul>
</div>

</div>

<div id="outline-container-31.1.2" class="outline-4">
<h4 id="sec-31.1.2">31.1.2 Discussão: </h4>
<div id="text-31.1.2">

<p>Os resultados corresponderam ao esperado, pois adicionar a probabilidade 1.0 a cada ação da política determinística deveria gerar um resultado equivalente na versão probabilística.
</p>

</div>
</div>

</div>

<div id="outline-container-31.2" class="outline-3">
<h3 id="sec-31.2">31.2 Experimentos de 51 a 55: repetição dos experimentos de 27 a 31 utilizando o PRQL probabilístico </h3>
<div id="text-31.2">


</div>

<div id="outline-container-31.2.1" class="outline-4">
<h4 id="sec-31.2.1">31.2.1 As políticas utilizadas são as ótimas com 1.0 para cada ação da política </h4>
<div id="text-31.2.1">

</div>

</div>

<div id="outline-container-31.2.2" class="outline-4">
<h4 id="sec-31.2.2">31.2.2 Utilizando como referência aprendizado com QL do Experimento 37 </h4>
<div id="text-31.2.2">

</div>
</div>

</div>

<div id="outline-container-31.3" class="outline-3">
<h3 id="sec-31.3">31.3 Experimentos de 56 a 66: teste de utilização incremental de política probabilística ótima versus <i>péssima</i> </h3>
<div id="text-31.3">


</div>

<div id="outline-container-31.3.1" class="outline-4">
<h4 id="sec-31.3.1">31.3.1 Problema: resolver task1 utilizando percentuais variáveis das políticas ótima e péssima </h4>
<div id="text-31.3.1">

</div>

</div>

<div id="outline-container-31.3.2" class="outline-4">
<h4 id="sec-31.3.2">31.3.2 Descrição: Rodar o PRQL com aproveitamento da política ótima e <i>péssima</i>, onde a <i>péssima</i> sugere sempre o oposto da ótima. </h4>
<div id="text-31.3.2">

</div>

</div>

<div id="outline-container-31.3.3" class="outline-4">
<h4 id="sec-31.3.3">31.3.3 A utilização da política ótima varia linearmente com o experimento, de 0% (Experimento 56) a 100% (Experimento 66). </h4>
<div id="text-31.3.3">

</div>

</div>

<div id="outline-container-31.3.4" class="outline-4">
<h4 id="sec-31.3.4">31.3.4 QLearning: foi realizado um novo aprendizado de Q-Learning para o experimento </h4>
<div id="text-31.3.4">

</div>
</div>
</div>
</div>
<div id="postamble"><p class="author"> Author: Rafael Lemes Beirigo
<a href="mailto:rafaelbeirigo@LTI-PROJETO-TM">&lt;rafaelbeirigo@LTI-PROJETO-TM&gt;</a>
</p>
<p class="date"> Date: 2012-04-10 16:06:18 BRT</p>
<p>HTML generated by org-mode 6.21b in emacs 23</p>
</div></body>
</html>
