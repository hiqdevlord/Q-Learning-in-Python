* Experimento 00 Experimento da seção 4.3 do artigo: resolver task \Omega
** 01 Utilizando \Pi_1, \Pi_2, \Pi_3, \Pi_4, \Pi_5
Esse experimento não está no artigo. Fiz para fins de testes.

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/01/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/01/w.png]]

** 02 Utilizando \Pi_2, \Pi_3, \Pi_4
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/02/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/02/w.png]]

** 03 Utilizando \Pi_1, \Pi_2, \Pi_3, \Pi_4
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/03/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/03/w.png]]

** 04 Utilizando \Pi_2, \Pi_3, \Pi_4, \Pi_5
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/04/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/04/w.png]]

** Discussão
Q-Learning apresentou um desempenho superior ao de PRQ-Learning.
Posteriormente descobri que o motivo era que a tabela Q era reiniciada somente para o 
PRQ-Learning, o que prejudicava o desempenho desse algoritmo.


* Experimento 01
Repetição de 00 após correção do problema.
** 01 Utilizando \Pi_1, \Pi_2, \Pi_3, \Pi_4, \Pi_5
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/01/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/01/01/w.png]]

** 02 Utilizando \Pi_2, \Pi_3, \Pi_4
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/02/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/01/02/w.png]]

** 03 Utilizando \Pi_1, \Pi_2, \Pi_3, \Pi_4
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/03/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/01/03/w.png]]

** 04 Utilizando \Pi_2, \Pi_3, \Pi_4, \Pi_5
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/04/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/01/04/w.png]]

** Discussão
O PRQL apresenta um desempenho similar ao do QL.
O resultado não foi o esperado: esperava-se um desempenho melhor por parte do PRQL,
dado que esse possui a reutilização de políticas.
Hipótese: não está utilizando as políticas antigas


* Experimento 02 QL vs PRQL no mundo 05x05
Motivo: queria rodar o PRQL sem a parte PR, ou seja, só utilizando
QLearning, pra ver se estava tudo OK.

Nesse experimento, o PRQL NÃO utilizava \pi-reuse, somente QL

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/02/w.png]]

** Discussão
PRQL apresentou um desempenho bastante similar ao do QL, o que sugere que a execução
do PRQL sem \pi-reuse equivale à execução do QL.

Isso já era esperado, pois o elemento do PRQL que acelera o aprendizado é justamente
o \pi-reuse


** 03 Repetição de 02, só que para o mundo 06x06
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/03/w.png]]

** Discussão
O resultado foi compatível com o obtido no experimento 02, conforme esperado


** 04 Repetição de 02, só que para a task \Omega do artigo
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/04/w.png]]

** Discussão:
   PRQL e QL apresentaram desempenhos compatíveis, o que era esperado


** 05 Repetição de 04, só que dessa vez ativando o \pi-reuse
A bilioteca de políticas continha somente a \Pi^*_\Omega.

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/05/w.png]]
  Sucesso: PRQL acelerou QLearning


** 06 Repetição de 05 para task \Omega do artigo reutilizando \Pi_2, \Pi_3 e \Pi_5 (são as que mais ajudam o agente)
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/06/w.png]]


** 09 Repetição de 02
#+ATTR_HTML: title="02" height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/02/w.png]]

#+ATTR_HTML: title="09" height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/09/w.png]]

** Discussão
O desempenho do PRQL aumentou em relação ao experimento 02. Isso pode
ser explicado pelo fato de que foi utilizado \pi-reuse nesse
experimento, o que contribui para acelerar o aprendizado.


** 23 Repetição do 02
#+ATTR_HTML: height="240" style="border:2px solid black;"


** 10 Repetição de 09, mas reutilizando uma política ótima para o problema de chegar à localização oposta (pior política que poderia reutilizar)
#+ATTR_HTML: title="09" height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/09/w.png]]

#+ATTR_HTML: title="10" height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/10/w.png]]

** Discussão
O resultado foi de acordo com o esperado, pois o desempenho do PRQL
cai quando a política que está sendo reutilizada atrapalharia na
solução do problema.

** Discussão
O elevado desempenho do PRQL pode ser explicado pelo fato de que a
política utilizada é justamente a ótima para o problema.

** Discussão
Problema: plotando W[ 1]

Foi plotado somente a recompensa acumulada quando se reutilizava uma
das políticas possíveis, L[ 1]para uma das políticas
reutilizadas.

Como espera-se um aumento gradual da utilização da política \Pi_new, e
a recompensa acumulada pela utilização de \Pi_new se encontra em W[
0], o valor plotado em W[ 1] não reflete o que esperamos.


** 07 Repetição de 06
A repetição foi feita para testes

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/07/w.png]]

** Discussão
Problema: plotando W[ 1]
A repetição foi feita antes da detecção do problema descrito em 05.


** 08 Repetição de 06, mas reutilizando somente a política ótima
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/08/w.png]]

** Discussão
O resultado foi diverso do esperado.

A recompensa acumulada estaciona em ~ 0.13, um valor extremamente
baixo, superado pelo Q-Learning durante os experimentos.


* 11 Resolver task \Omega utilizando  \Pi_2, \Pi_3, \Pi_4, \Pi_5
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/11/w.png]]

** Discussão
PRQL apresenta desempenho inferior ao de QL, o oposto do esperado.

** 12 Repetição
Reutilizando somente a policy obtida em 11 pelo QLearning (ótima para o problema)
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/12/w.png]]

** 13 Repetição
Chamei o solveMDP... pra criar os arquivos (tirar a dúvida se arquivos estão corretos)
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/13/w.png]]
Pude perceber a partir desse experimento que as políticas que estavam
sendo reutilizadas eram subótimas.

** 14 Repetição
Agora utilizando a política ótima
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/14/w.png]]

** 20 Repetição
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/20/w.png]]

** 25 Repetição
Repetição do 20
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/25/w.png]]


* Obtenção das políticas ótimas para as tasks de 1 a 5
** 15 Obtenção de \Pi_1
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/15/w.png]]
  Consumo de tempo: ~ 10'


** 16 Obtenção de \Pi_2
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/16/w.png]]
  Consumo de tempo: ~ 10'


** 17 Obtenção de \Pi_3
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/17/w.png]]
  Consumo de tempo: ~ 10'


** 18 Obtenção de \Pi_4
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/18/w.png]]
  Consumo de tempo: ~ 10'


** 19 Obtenção de \Pi_5
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/19/w.png]]
  Consumo de tempo: ~ 10'


* 21 Resolver task \Omega utilizando \Pi_2, \Pi_3, \Pi_4
** 26 Repetição do 21
#+ATTR_HTML: height="240" style="border:2px solid black;"



* 22 Resolver task \Omega utilizando \Pi_1, \Pi_2, \Pi_3, \Pi_4
** 24 Repetição do 22
#+ATTR_HTML: height="240" style="border:2px solid black;"


* Aprendizado de \Pi_\Omega com reutilização individual de políticas
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/27/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/27/w.png]]

No gráfico, os valores referentes ao Q-Learning foram obtidos no experimento 37.

** 27 Reutilizando \Pi_1
Consumo de tempo: 5m20.356s
** 28 Reutilizando \Pi_2
Consumo de tempo: 7m53.056s
** 29 Reutilizando \Pi_3
Consumo de tempo: 9m8.582s
** 30 Reutilizando \Pi_4
Consumo de tempo: 10m19.403s
** 31 Reutilizando \Pi_5
Consumo de tempo: 6m8.686s


* Resolver task1 utilizando \pi-reuse(\Pi_1)
** 10 execuções
Política reutilizada: \Pi_1, obtida no experimento 15

w.32.png - dados do Q-Learning obtidos no experimento 32
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/32/w.32.png]]

w.37.png - dados do Q-Learning obtidos no experimento 37
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/32/w.37.png]]

*** Experimento 32 - em 100% dos episódios - 10 execuções
*** Experimento 33 - em  75% dos episódios - 10 execuções
*** Experimento 34 - em  50% dos episódios - 10 execuções
*** Experimento 35 - em  25% dos episódios - 10 execuções
*** Experimento 36 - em   0% dos episódios - 10 execuções

** 100 execuções
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/37/w.png]]

Política reutilizada: \Pi_1, obtida no experimento 15

Dados do Q-Learning obtidos no experimento 37

*** Experimento 37 - em   0% dos episódios
*** Experimento 38 - em  25% dos episódios
*** Experimento 39 - em  50% dos episódios
*** Experimento 40 - em  75% dos episódios
*** Experimento 41 - em 100% dos episódios


* Testes da versão probabilística do PRQL (PRQL_prob)
** PRQL_prob /versus/ PRQL
*** Conservador: política determinística com 1.0 em tudo
Reutilizar uma política determinística ótima e sua versão probabilística (1.0 de probabilidade para cada ação ótima)

**** Task 1 reutilizando \Pi^*_1
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/49/w.png]]

***** Experimentos
****** 49
******* Algoritmo: PRQL_prob
******* Task: 1
******* Políticas reutilizadas: \Pi_1^*_prob1
******** \Pi_1^*_prob1 foi obtida colocando 1.0 em cada linha da política ótima determinística induzida por \Pi_1^*_prob
******* Parâmetros: [[/home/rafaelbeirigo/ql/experiments/49/PRQL/parameters.out]]

****** 50
******* Algoritmo: PRQL
******* Task: 1
******* Políticas reutilizadas: \Pi_1^*
******* Parâmetros: [[/home/rafaelbeirigo/ql/experiments/50/PRQL/parameters.out]]
******* Discussão:
Os resultados corresponderam ao esperado, pois adicionar a probabilida
de 1.0 a cada ação da política determinística deveria gerar um resultado
equivalente na versão probabilística.


**** Task \Omega reutilizando \Pi_2, \Pi_3, \Pi_4, \Pi_5
***** PRQL
#+ATTR_HTML: height="320" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/27/w.png]]

****** Experimentos: 27 a 31
****** Observações: Utilizando como referência aprendizado com QL do Experimento 37


***** PRQL_prob
****** Experimentos: 51 a 55
#+ATTR_HTML: height="320" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/51/w.png]]


*** Menos conservador: PRQL_prob vs QL
**** Task 1 cada vez mais probabilístico
Utilização incremental de política probabilística ótima /versus/ /péssima/
(PRQL_prob cada vez mais prob)

***** Algoritmo: PRQL_prob
***** Políticas reutilizadas:
A partir de  \Pi_1, foi gerada uma nova política, em que para cada par estado-ação (s, a), geraram-se duas ênuplas:

/s, a, p/

e

/s, a^{-1}, (1 - p)/, em que 

/a^{-1}/ é a ação /inversa/ de /a/, ou seja, se /a/ = /East/, /a^{-1}/ = /West/, se  /a/ = /North/, /a^{-1}/ = /South/, e assim por diante.

O valor de p varia por experimento, valendo /0.1/ no experimento /56/, /0.1/ no experimento /57/, e assim por diante, até atingir /1.0/ no experimento /66/.

No experimento /57/, /a/ vale /0.0/, logo, a linha /s, a, p/ é omitida.

Um análogo disso ocorre para o experimento /66/, onde /p/ vale /1.0/, portanto /1 - p/ = /0/ e, dessa forma, a linha /s, a^{-1}, (1 - p)/ pode ser omitida.

Exemplo:

/row1col1 East/ *(linha original na \Pi_1)*

linhas geradas a partir dessa:

/row1col1 East 0.7/ *(ação ótima, por ter a maior probabilidade de escolha originalmente em \Pi_1 - 30% de chance de ser a escolhida)*

/row1col1 East 0.3/ *(ação "péssima" - 70% de chance de ser a escolhida)*

Ou seja, geramos políticas probabilísticas que variam da pior possível (a /péssima/), que somente possui ações opostas àquelas da política ótima, até a ótima.

Intermediariamente, temos políticas "sujas", onde as ações ótimas são intercaladas por ações /péssimas/.

Na tabela abaixo, temos a listagem completa dos valores de /p/ para cada experimento realizado.

| Experimento | Percentual de uso da ação /ótima/ (/p/) | Percentual de uso da ação /péssima/ /(1 - p)/ |
|-------------+-----------------------------------------+-----------------------------------------------|
|          56 |                                      0% |                                          100% |
|          57 |                                     10% |                                           90% |
|          58 |                                     20% |                                           80% |
|          59 |                                     30% |                                           70% |
|          60 |                                     40% |                                           60% |
|          61 |                                     50% |                                           50% |
|          62 |                                     60% |                                           40% |
|          63 |                                     70% |                                           30% |
|          64 |                                     80% |                                           20% |
|          65 |                                     90% |                                           10% |
|          66 |                                    100% |                                            0% |

***** Experimentos
****** 10 execuções
******* 56 a 66
#+ATTR_HTML: height="640" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/56/w.png]]

******** Parâmetros: [[/home/rafaelbeirigo/ql/experiments/56/PRQL/parameters.out]]
******** QLearning: 56
******** Discussão:
Podemos verificar que a política que gerou o melhor resultado na reutilização
foi a que possui 70% de /ótimo/ e 30% de /péssimo/ (\Pi_{70-30}).

Isso foi uma surpresa, já que o natural seria esperar que a reutilização de
uma política que contenha somente ações ótimas gerasse um desempenho melhor
do que a reutilização de uma política que contivesse 30% de ações /péssimas/.

Entretanto podemos ver que a \Pi_{70-30} possui um /jumpstart/ significativo,
o que poderia jogar a média de W (que é justamente o que é mostrado no gráfico)
para cima.

Para testar se esse foi realmente o motivo, o experimento foi repetido, só que
dessa vez com 1000 execuções ao invés de 100 (experimentos de /67/ a /77/ e /90/
a /100/).

Com isso, esperamos diminuir o impacto que a /sorte/ de ter tido um bom
desempenho nos episódios iniciais pudesse ter sobre a recompensa média
alcançada.

Isso foi feito nos experimentos de /67/ a /77/.

******* 101 a 111
#+ATTR_HTML: height="640" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/101/w.png]]

******** Parâmetros: [[/home/rafaelbeirigo/ql/experiments/101/PRQL/parameters.out]]
******** QLearning: 101
******** Discussão:
Esse experimento é uma repetição do /56/ a /66/, para testar se está tudo OK.
O resultado correspondeu ao esperado.

******* 112 a 122
#+ATTR_HTML: height="640" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/112/w.png]]

******** Parâmetros: [[/home/rafaelbeirigo/ql/experiments/112/PRQL/parameters.out]]
******** QLearning: 112
******** Discussão:
Esse experimento foi realizado para testar se o /merge/ via /git/ do /branch/
/probabilistic/ havia sido realizado com sucesso.

Os resultados estão muito próximos dos obtidos no mesmo experimento quando
executados com a versão anterior ao /merge/, o que sugere que tenha ocorrido
tudo bem no processo de /merge/.

******* 123 a 133
#+ATTR_HTML: height="640" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/123/w.png]]

******** Parâmetros: [[/home/rafaelbeirigo/ql/experiments/123/PRQL/parameters.out]]
******** QLearning: 123
******** Discussão:
Esse experimento foi realizado para testar se a estratégia de full-greedy implica
em alguma melhoria no desempenho do algoritmo.

O resultado esperado é que não haja melhorias, pelo contrário, que o fato de o
agente não poder realizar a exploração durante o episódio de Q-Learning implique
em uma queda do desempenho no aprendizado.

Algo que pude observar foi uma marcante queda na quantidade de /policy-reuse/:
| Execução | Policy Reuse | Q-Learning |
|----------+--------------+------------|
|        1 |         1958 |         42 |
|        2 |         1958 |         42 |
|        3 |         1951 |         49 |
|        4 |         1949 |         51 |
|        5 |         1946 |         54 |
|        6 |         1952 |         48 |
|        7 |         1957 |         43 |
|        8 |         1963 |         37 |
|        9 |         1950 |         50 |
|       10 |         1935 |         65 |

Podemos verificar que o resultado foi o oposto do esperado, e houve um aumento
significativo do desempenho do agente no aprendizado.

Dúvidas que surgiram:

1) Por que isso aconteceu?

2) Qual foi a modificação, na média, de aplicação de \pi-reuse?

3) Por que a política com 100% de ótimo não teve o melhor desempenho de todas?

****** 100 execuções
******* 79 a 89
#+ATTR_HTML: height="640" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/79/w.png]]

******** Parâmetros: [[/home/rafaelbeirigo/ql/experiments/79/PRQL/parameters.out]]
******** QLearning: 79

******** Discussão:
O resultado correspondeu ao esperado.

****** 1000 execuções
******* 67 a 77
#+ATTR_HTML: height="640" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/67/w.png]]

******** Parâmetros: [[/home/rafaelbeirigo/ql/experiments/67/PRQL/parameters.out]]
******** QLearning: 67
******** Discussão:
O objetivo desse experimento era veriricar se o melhor desempenho obtido
com a reutilização de uma política com 30% de ações /péssimas/ poderia
ser explicado por um desempenho extremamente bom no início, que jogaria
a média /para cima/.

RESULTADOS: Podemos ver que os resultados corresponderam ao esperado, ou seja, quanto
mais probabilidade o agente tem de reutilizar uma ação ótima através do
\pi-reuse, melhor é o seu desempenho no aprendizado (medido pela média
cumulativa do /W/).

Futuramente, pude ver que meu erro na verdade foi não ter percebido que
o experimento correspondente ao /70-3-/ na verdade terminou anormalmente.
Dessa forma, o /w.out/ plotado correspondia a um experimento realizado
anteriormente, o que explica seu comportamento de /outlier/.

Através da análise do gráfico, podemos ver que uma adição igual ou superior
a 40% de probabilidade de utilização de ações /péssimas/ implica em um
desempenho inferior ao da execução do QL.

******* 90 a 100
#+ATTR_HTML: height="640" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/90/w.png]]

******** Parâmetros: [[/home/rafaelbeirigo/ql/experiments/90/PRQL/parameters.out]]
******** QLearning: 90
******** Discussão:
O resultado correspondeu ao esperado.

******* 139 a 149
#+ATTR_HTML: height="640" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/139/w.png]]

******** Parâmetros: [[/home/rafaelbeirigo/ql/experiments/139/PRQL/parameters.out]]
******** QLearning: 139
******** Discussão:
Esse experimento foi realizado para verificar o impacto da correção em relação ao
/full-greedy/ (detalhes no grupo de experimentos de /123/ a /133/).


* IPMU
** 42 Com arquivos originais - 2000 episódios, 100 passos
w.out completamente zerado.


** 43 Com arquivos originais - 1e05 episódios, 1000 passos
Supondo que o problema relatado em 42 fosse a quantidade de episódios
e/ou passos, rodei novamente, com 


** 44 Modifiquei o transitions.in
   Modificação realizada:
   Para cada linha do arquivo, o valor da transição foi alterado para
   1.0 / 136, sendo que 136 é o número total de estados.
Com isso, espero ter um grafo completo de transições, logo, poderei
   verificar se foi esse o problema que impediu o agente de receber
   recompensas nos experimentos 42 e 43.
Vale notar que estou supondo que existe uma linha s a s' t para todas
   as combinações de s a s' possíveis.
É bem provável que isso seja verdade já que a quantidade de linhas do
   arquivo transitions.in é 73984 = 136 * 4 * 136 (|S| * |A| * |S|).


** 78 Reutilizando políticas probabilísticas
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/78/w.png]]

*** PRQL
**** /prob/: reutilizando pol. prob. enviada pelo Marcelo
***** Algoritmo: PRQL_prob
***** Task: IPMU
***** Políticas reutilizadas: \Pi_IPMU_prob
***** Parâmetros: [[/home/rafaelbeirigo/ql/experiments/78/PRQL/prob/parameters.out]]

**** /prob.det/: pol. ótima induzida pela pol. enviada pelo Marcelo
***** Algoritmo: PRQL_prob
***** Task: IPMU
***** Políticas reutilizadas: \Pi_IPMU_prob1 (1.0 para cada ação "ótima" (a que tinha a maior probabilidade no arquivo original))
***** Parâmetros: [[/home/rafaelbeirigo/ql/experiments/78/PRQL/prob.det/parameters.out]]

**** /det/: reutilizando pol. ótima induzida, só que na versão antiga do PRQL (a versão determinística)
***** Algoritmo: PRQL
***** Task: IPMU
***** Políticas reutilizadas: \Pi_IPMU_det
***** Parâmetros: [[/home/rafaelbeirigo/ql/experiments/78/PRQL/det/parameters.out]]

*** QL
***** Task: IPMU
***** Parâmetros: [[/home/rafaelbeirigo/ql/experiments/78/QL/parameters.out]]


* Repetindo após correção /full-greedy/
Percebi  que houve uma melhora significativa no desempenho do aprendizado
e estou repetindo alguns experimentos cujos resultados foram díspares do
esperado.

Os experimentos que marcaram a descoberta foram os do grupo de 123 a 133.

** Aprendizado de \Pi_\Omega com reutilização individual de políticas
Esse experimento me deixou bastante intrigado, porque o resultado foi
significativamente (quanto?) díspare do apresentado no artigo.

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/27/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/27/w.png]]

No gráfico, os valores referentes ao Q-Learning foram obtidos no experimento 37.

*** 134 Reutilizando \Pi_1
*** 135 Reutilizando \Pi_2
*** 136 Reutilizando \Pi_3
*** 137 Reutilizando \Pi_4
*** 138 Reutilizando \Pi_5
