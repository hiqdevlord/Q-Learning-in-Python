* Experimento 00 Experimento da seção 4.3 do artigo: resolver task \Omega
** 01 Utilizando \Pi_1, \Pi_2, \Pi_3, \Pi_4, \Pi_5
Esse experimento não está no artigo. Fiz para fins de testes.

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/01/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/01/w.png]]

** 02 Utilizando \Pi_2, \Pi_3, \Pi_4
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/02/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/02/w.png]]

** 03 Utilizando \Pi_1, \Pi_2, \Pi_3, \Pi_4
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/03/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/03/w.png]]

** 04 Utilizando \Pi_2, \Pi_3, \Pi_4, \Pi_5
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/04/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/04/w.png]]

** Discussão
Q-Learning apresentou um desempenho superior ao de PRQ-Learning.
Posteriormente descobri que o motivo era que a tabela Q era reiniciada somente para o 
PRQ-Learning, o que prejudicava o desempenho desse algoritmo.


* Experimento 01
Repetição de 00 após correção do problema.
** 01 Utilizando \Pi_1, \Pi_2, \Pi_3, \Pi_4, \Pi_5
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/01/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/01/01/w.png]]

** 02 Utilizando \Pi_2, \Pi_3, \Pi_4
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/02/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/01/02/w.png]]

** 03 Utilizando \Pi_1, \Pi_2, \Pi_3, \Pi_4
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/03/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/01/03/w.png]]

** 04 Utilizando \Pi_2, \Pi_3, \Pi_4, \Pi_5
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/00/04/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/01/04/w.png]]

** Discussão
O PRQL apresenta um desempenho similar ao do QL.
O resultado não foi o esperado: esperava-se um desempenho melhor por parte do PRQL,
dado que esse possui a reutilização de políticas.
Hipótese: não está utilizando as políticas antigas


* Experimento 02 QL vs PRQL no mundo 05x05
Motivo: queria rodar o PRQL sem a parte PR, ou seja, só utilizando
QLearning, pra ver se estava tudo OK.

Nesse experimento, o PRQL NÃO utilizava \pi-reuse, somente QL

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/02/w.png]]

** Discussão
PRQL apresentou um desempenho bastante similar ao do QL, o que sugere que a execução
do PRQL sem \pi-reuse equivale à execução do QL.

Isso já era esperado, pois o elemento do PRQL que acelera o aprendizado é justamente
o \pi-reuse


* 03 Repetição de 02, só que para o mundo 06x06
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/03/w.png]]

** Discussão
O resultado foi compatível com o obtido no experimento 02, conforme esperado


* 04 Repetição de 02, só que para a task \Omega do artigo
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/04/w.png]]

** Discussão:
   PRQL e QL apresentaram desempenhos compatíveis, o que era esperado


* 05 Repetição de 04, só que dessa vez ativando o \pi-reuse
A bilioteca de políticas continha somente a \Pi^*_\Omega.

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/05/w.png]]
  Sucesso: PRQL acelerou QLearning

** Discussão
O elevado desempenho do PRQL pode ser explicado pelo fato de que a
política utilizada é justamente a ótima para o problema.


* 06 Repetição de 05 para task \Omega do artigo reutilizando \Pi_2, \Pi_3 e \Pi_5 (são as que mais ajudam o agente)
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/06/w.png]]

** Discussão
Problema: plotando W[ 1]

Foi plotado somente a recompensa acumulada quando se reutilizava uma
das políticas possíveis, L[ 1]para uma das políticas
reutilizadas.

Como espera-se um aumento gradual da utilização da política \Pi_new, e
a recompensa acumulada pela utilização de \Pi_new se encontra em W[
0], o valor plotado em W[ 1] não reflete o que esperamos.


* 07 Repetição de 06
A repetição foi feita para testes

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/07/w.png]]

** Discussão
Problema: plotando W[ 1]
A repetição foi feita antes da detecção do problema descrito em 05.


* 08 Repetição de 06, mas reutilizando somente a política ótima
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/08/w.png]]

** Discussão
O resultado foi diverso do esperado.

A recompensa acumulada estaciona em ~ 0.13, um valor extremamente
baixo, superado pelo Q-Learning durante os experimentos.


* 09 Repetição de 02
#+ATTR_HTML: title="02" height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/02/w.png]]

#+ATTR_HTML: title="09" height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/09/w.png]]

** Discussão
O desempenho do PRQL aumentou em relação ao experimento 02. Isso pode
ser explicado pelo fato de que foi utilizado \pi-reuse nesse
experimento, o que contribui para acelerar o aprendizado.


* 10 Repetição de 09, mas reutilizando uma política ótima para o problema de chegar à localização oposta (pior política que poderia reutilizar)
#+ATTR_HTML: title="09" height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/09/w.png]]

#+ATTR_HTML: title="10" height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/10/w.png]]

** Discussão
O resultado foi de acordo com o esperado, pois o desempenho do PRQL
cai quando a política que está sendo reutilizada atrapalharia na
solução do problema.


* 11 Resolver task \Omega utilizando  \Pi_2, \Pi_3, \Pi_4, \Pi_5
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/11/w.png]]

** Discussão
PRQL apresenta desempenho inferior ao de QL, o oposto do esperado.


* 12 Repetição de 11 reutilizando somente a policy obtida em 11 pelo QLearning (ótima para o problema)
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/12/w.png]]



* 13 Repetição de 12, só que chamei o solveMDP... pra criar os arquivos (tirar a dúvida se
  arquivos estão corretos)
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/13/w.png]]
Pude perceber a partir desse experimento que as políticas que estavam
sendo reutilizadas eram subótimas.


* 14 Repetição do 13, só que agora utilizando a política ótima
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/14/w.png]]


* 15 Obtenção de \Pi_1
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/15/w.png]]
  Consumo de tempo: ~ 10'


* 16 Obtenção de \Pi_2
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/16/w.png]]
  Consumo de tempo: ~ 10'


* 17 Obtenção de \Pi_3
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/17/w.png]]
  Consumo de tempo: ~ 10'


* 18 Obtenção de \Pi_4
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/18/w.png]]
  Consumo de tempo: ~ 10'


* 19 Obtenção de \Pi_5
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/19/w.png]]
  Consumo de tempo: ~ 10'


* 20 Resolver task \Omega utilizando \Pi_2, \Pi_3, \Pi_4, \Pi_5 (Repetição do 11)
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/20/w.png]]


* 21 Resolver task \Omega utilizando \Pi_2, \Pi_3, \Pi_4


* 22 Resolver task \Omega utilizando \Pi_1, \Pi_2, \Pi_3, \Pi_4


* 23 Repetição do 02
#+ATTR_HTML: height="240" style="border:2px solid black;"


* 24 Repetição do 22
#+ATTR_HTML: height="240" style="border:2px solid black;"


* 25 Repetição do 20
#+ATTR_HTML: height="240" style="border:2px solid black;"


* 26 Repetição do 21
#+ATTR_HTML: height="240" style="border:2px solid black;"


* Aprendizado de \Pi_\Omega com reutilização individual de políticas
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/27/map.png]]

#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/27/w.png]]

No gráfico, os valores referentes ao Q-Learning foram obtidos no experimento 37.

** 27 Reutilizando \Pi_1
Consumo de tempo: 5m20.356s
** 28 Reutilizando \Pi_2
Consumo de tempo: 7m53.056s
** 29 Reutilizando \Pi_3
Consumo de tempo: 9m8.582s
** 30 Reutilizando \Pi_4
Consumo de tempo: 10m19.403s
** 31 Reutilizando \Pi_5
Consumo de tempo: 6m8.686s


* Resolver task1 utilizando \pi-reuse(\Pi_1)
** 10 execuções
Política reutilizada: \Pi_1, obtida no experimento 15

w.32.png - dados do Q-Learning obtidos no experimento 32
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/32/w.32.png]]

w.37.png - dados do Q-Learning obtidos no experimento 37
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/32/w.37.png]]

*** Experimento 32 - em 100% dos episódios - 10 execuções
*** Experimento 33 - em  75% dos episódios - 10 execuções
*** Experimento 34 - em  50% dos episódios - 10 execuções
*** Experimento 35 - em  25% dos episódios - 10 execuções
*** Experimento 36 - em   0% dos episódios - 10 execuções

** 100 execuções
#+ATTR_HTML: height="240" style="border:2px solid black;"
[[/home/rafaelbeirigo/ql/experiments/37/w.png]]

Política reutilizada: \Pi_1, obtida no experimento 15

Dados do Q-Learning obtidos no experimento 37

*** Experimento 37 - em   0% dos episódios
*** Experimento 38 - em  25% dos episódios
*** Experimento 39 - em  50% dos episódios
*** Experimento 40 - em  75% dos episódios
*** Experimento 41 - em 100% dos episódios


* Aprendizado de política para o problema enviado pelo Marcelo (02/Abril)
** 42 Com arquivos originais - 2000 episódios, 100 passos
w.out completamente zerado.

** 43 Com arquivos originais - 1e05 episódios, 1000 passos
Supondo que o problema relatado em 42 fosse a quantidade de episódios
e/ou passos, rodei novamente, com 

** 44 Modifiquei o transitions.in
   Modificação realizada:
   Para cada linha do arquivo, o valor da transição foi alterado para
   1.0 / 136, sendo que 136 é o número total de estados.
Com isso, espero ter um grafo completo de transições, logo, poderei
   verificar se foi esse o problema que impediu o agente de receber
   recompensas nos experimentos 42 e 43.
Vale notar que estou supondo que existe uma linha s a s' t para todas
   as combinações de s a s' possíveis.
É bem provável que isso seja verdade já que a quantidade de linhas do
   arquivo transitions.in é 73984 = 136 * 4 * 136 (|S| * |A| * |S|).
